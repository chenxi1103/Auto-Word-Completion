import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.*;

public class LanguageModel {
	public static class Map extends Mapper<LongWritable, Text, Text, Text> {

		int threashold;
		@Override
		public void setup(Context context) {
			// Get the value from the command line by conf2
			Configuration conf2 = context.getConfiguration();
			// Set the default value tobe 30
			threshold = conf2.getInt("threshold", 30);
		}

		/*
		 * Mapper Method:
		 * Input is from the result that generated by first job stored in HDFS
		 * Specifically, "key" still represents the position of this phrase in the file, which is useless here.
		 * "value" store both the perticular phrase and its total count. (Remember, in MapReduce, the key-value are saparated by "\t")
		 * For example, here, the value might be "I love big data\t100"
		 */
		@Override
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
			if((value == null) || (value.toString().trim()).length() == 0) {
				return;
			}
			// First, separate the key and value by "\t"
			String line = value.toString().trim();
			String[] wordsPlusCount = line.split("\t");
			// Check if the separated result is no less than 2.
			if(wordsPlusCount.length < 2) {
				return;
			}
			// Check if the total count is less than threshold
			if(Integer.valueOf(wordsPlusCount[1]) < threshold){
				return;
			}
			// Saparate the phrase into two part: first part represents the possible user input phrase
			// Second part, which is also the last word of the original phrase, represents the possible
			// following phrase of the input phrase
			String[] words = wordsPlusCount[0].split("\\s+");
			Stringbuffer sb = new StringBuffer();
			for(int i = 0; i < words.length-1; i++){
				sb,append(word[i] + " ");
			}
			// Remember to trim since there is an extra space at the end
			String outputKey = sb.toString().trim();
			// New key - possible user input phrase
			int count = Integer.valueOf(wordsPlusCount[1]);
			// New Value - possible following phrase and its total count
			String outputValue = words[words.length-1] + "=" + count;
			context.write(Text(outputKey), Text(outputValue));
		}
	}

	public static class Reduce extends Reducer<Text, Text, DBOutputWritable, NullWritable> {

		int k;
		// get the n (topN) from the conf2
		@Override
		public void setup(Context context) {
			Configuration conf2 = context.getConfiguration();
			k = conf.getInt("n", 5);
		}

		@Override
		public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
			// Use TreeMap(BST) to do topK sort (Time(nlog(n))/Space(n) complexity a little bit worse than priority queue(Time:nlog(k)/Space:K))
			// Use the number of count to be the key, value is a list of the words that have the same total count
			// Remember to do reverseOrder since we want the largest value to be the first one.
			TreeMap<Integer, List<String>> treemap = new TreeMap<Integer, List<String>>(Collections.<Integer>reverseOrder());

			// Iterate the values
			for(Text value: values){
				String currValue = value.toString().trim();
				String splited = currValue.split("=");
				// Get word and count
				String word = splited[0];
				int count = Integer.parseInt(splited[1]);
				// Check if the TreeMap has already have this count as key
				if(treemap.containsKey(count)){
					treemap.get(count).add(word);
				}else{
					List<String> list = new ArrayList<String>();
					list.add(word);
					treemap.put(count,list);
				}
			}

			// After interate the values, the sort has been done at the same time
			// Iterate the treemap and Only persist the topK values in treemap
			Iterator<Integer> iterator = treemap.keySet().iterator();
			// Here we do not do i++ in the for loop head since we not only should do i++ when
			// we read a new key, but also when reading every new word in the list in the value.
			// So we manually do i++ in the for loop when we read a new word in the list.
			for(int i = 0; iterator.hasNaxt() && i < k;){
				int count = iterator.next();
				List<String> words = treemap.get(count);
				for(String word : words){
					// Write the word into database by implementing DBWritable class
					// Write all the data as key of the context, if there is nothing to write as the value of the context, set it to Null
					context.write(new DBOutputWritable(key.toString(), word, count),NullWritable.get());
					i++;
				}
			}
		}
	}
}
